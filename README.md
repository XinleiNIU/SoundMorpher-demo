## This is a demo page for our paper: SoundMorpher: Perceptually-Uniform Sound Morphing with Diffusion Model

### About

This is a [demo](https://XinleiNIU.github.io/SoundMorpher-demo/) for our paper '_SoundMorpher: Perceptually-Uniform Sound Morphing with Diffusion Model_'. 

### Abstract

We present SoundMorpher, a sound morphing method that generates perceptually uniform morphing trajectories using a diffusion model. Traditional sound morphing methods models the intractable relationship between morph factor and perception of the stimuli for resulting sounds under a linear assumption, which oversimplifies the complex nature of sound perception and limits their morph quality. In contrast, SoundMorpher explores an explicit proportional mapping between the morph factor and the perceptual stimuli of morphed sounds based on Mel-spectrogram. This approach enables smoother transitions between intermediate sounds and ensures perceptually consistent transformations, which can be easily extended to diverse sound morphing tasks. Furthermore, we present a set of quantitative metrics to comprehensively assess sound morphing systems based on three objective criteria, namely, correspondence, perceptual intermediateness, and smoothness. We provide extensive experiments to demonstrate the effectiveness and versatility of SoundMorpher in real-world scenarios, highlighting its potential impact on various applications such as creative music composition, film post-production and interactive audio technologies.

### Citation

If you are interesting in our work, please cite it as below:

```
@article{niu2024soundmorpher,
  title={SoundMorpher: Perceptually-Uniform Sound Morphing with Diffusion Model},
  author={Niu, Xinlei and Zhang, Jing and Martin, Charles Patrick},
  year={2024}
}
```
